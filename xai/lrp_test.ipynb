{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, Grayscale\n",
    "\n",
    "from zennit.attribution import Gradient, SmoothGrad\n",
    "from zennit.torchvision import ResNetCanonizer\n",
    "from zennit.composites import EpsilonGammaBox, EpsilonPlusFlat, EpsilonAlpha2Beta1Flat, EpsilonAlpha2Beta1, EpsilonPlus\n",
    "from zennit.image import imgify, imsave\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tempo.models import NewTempoLinear\n",
    "from tempo.data.datasets import finetune_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"test_18.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "weights_tp = torch.load(f'../model_zoo/{MODEL_NAME}')\n",
    "model_tp = NewTempoLinear(out_features=24, weights=None,freeze_backbone=True)\n",
    "model_tp.load_state_dict(weights_tp)\n",
    "model_tp.requires_grad = True\n",
    "_ = model_tp.eval()\n",
    "\n",
    "weights_bl = torch.load('../model_zoo/baseline.pth')\n",
    "model_bl = NewTempoLinear(out_features=24, weights=None,freeze_backbone=True)\n",
    "model_bl.load_state_dict(weights_bl)\n",
    "model_bl.requires_grad = True\n",
    "_ = model_bl.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance(model, data, num_classes):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(model(data).flatten()).item()\n",
    "\n",
    "    canonizer = ResNetCanonizer()\n",
    "\n",
    "    # create a composite, specifying the canonizers\n",
    "    composite = EpsilonPlusFlat(canonizers=[canonizer])\n",
    "\n",
    "    # choose a target class for the attribution (label 437 is lighthouse)\n",
    "    target = torch.eye(num_classes)[[pred]]\n",
    "\n",
    "    # create the attributor, specifying model and composite\n",
    "    with Gradient(model=model, composite=composite) as attributor:\n",
    "        # compute the model output and attribution\n",
    "        output, attribution = attributor(data, target)\n",
    "\n",
    "    relevance = attribution.sum(1)\n",
    "\n",
    "    return pred, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test-set\n",
    "test_loader = finetune_dataset2(train=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # imagenet normalization, also applied during tempo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(200., 200.))\n",
    "# grid = ImageGrid(fig, 111,\n",
    "#                  nrows_ncols=(test_loader.dataset.__len__(), 3),\n",
    "#                  axes_pad=0.5,\n",
    "#                  )\n",
    "\n",
    "# grid = chunker(grid, 3)\n",
    "# for ax, (im, lbl) in zip(grid, test_loader):\n",
    "#     img_n = normalize(im)\n",
    "#     pred, relevance = get_relevance(model_tp, img_n, 24)\n",
    "#     pred_bl, relevance_bl = get_relevance(model_bl, img_n, 24)\n",
    "#     img_hm = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "#     img_hm_bl = imgify(relevance_bl, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "#     ax[0].set_title(f'Target: {lbl.item()}')\n",
    "#     ax[1].set_title(f'Prediction: {pred}')\n",
    "#     ax[2].set_title(f'Prediction: {pred_bl}')\n",
    "\n",
    "#     im = im[0]\n",
    "#     ax[0].imshow(im.permute(1,2,0))\n",
    "#     ax[1].imshow(img_hm)\n",
    "#     ax[2].imshow(img_hm_bl)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "            'A': 0, \n",
    "            'B': 1, \n",
    "            'C': 2,\n",
    "            'D': 3,\n",
    "            'E': 4,\n",
    "            'F': 5,\n",
    "            'G': 6,\n",
    "            'H': 7,\n",
    "            'I': 8,\n",
    "            'K': 9,\n",
    "            'L': 10,\n",
    "            'M': 11,\n",
    "            'N': 12,\n",
    "            'O': 13,\n",
    "            'P': 14,\n",
    "            'Q': 15,\n",
    "            'R': 16,\n",
    "            'S': 17,\n",
    "            'T': 18,\n",
    "            'U': 19,\n",
    "            'V': 20,\n",
    "            'W': 21,\n",
    "            'X': 22,\n",
    "            'Y': 23,\n",
    "            }\n",
    "\n",
    "inv_class_map = {v: k for k, v in class_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_new(model, data, num_classes):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, (idx1, idx2, idx3) = torch.topk(model(data).flatten(), 3)\n",
    "\n",
    "    canonizer = ResNetCanonizer()\n",
    "\n",
    "    # low, high = normalize(torch.tensor([[[[[0.]]] * 3], [[[[1.]]] * 3]]))\n",
    "    # composite = EpsilonGammaBox(low=low, high=high, canonizers=[canonizer])\n",
    "\n",
    "    # create a composite, specifying the canonizers\n",
    "    composite = EpsilonPlusFlat(canonizers=[canonizer])\n",
    "\n",
    "    # composite = EpsilonAlpha2Beta1(canonizers=[canonizer])\n",
    "    # composite = EpsilonAlpha2Beta1Flat(canonizers=[canonizer])\n",
    "    \n",
    "\n",
    "\n",
    "    # choose a target class for the attribution\n",
    "    target1 = torch.eye(num_classes)[[idx1.item()]]\n",
    "    # target2 = torch.eye(num_classes)[[idx2.item()]]\n",
    "    # target3 = torch.eye(num_classes)[[idx3.item()]]\n",
    "\n",
    "    # create the attributor, specifying model and composite\n",
    "    with Gradient(model=model, composite=composite) as attributor:\n",
    "        # compute the model output and attribution\n",
    "        output1, attribution1 = attributor(data, target1)\n",
    "        # output2, attribution2 = attributor(data, target2)\n",
    "        # output3, attribution3 = attributor(data, target3)\n",
    "\n",
    "    relevance1 = attribution1.sum(1)\n",
    "    # relevance2 = attribution2.sum(1)\n",
    "    # relevance3 = attribution3.sum(1)\n",
    "\n",
    "    return (idx1, relevance1)#, (idx2, relevance2), (idx3, relevance3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(250., 250.))\n",
    "grid = ImageGrid(fig, 111,\n",
    "                 nrows_ncols=(test_loader.dataset.__len__(), 3),\n",
    "                 axes_pad=0.5,\n",
    "                 )\n",
    "\n",
    "grid = chunker(grid, 3)\n",
    "for ax, (im, lbl) in zip(grid, test_loader):\n",
    "    img_n = normalize(im)\n",
    "    # (pred1, relevance1), (pred2, relevance2), (pred3, relevance3) = get_relevance_new(model_tp, img_n, 24)\n",
    "    # (pred_bl_1, relevance_bl_1), (pred_bl_2, relevance_bl_2), (pred_bl_3, relevance_bl_3) = get_relevance_new(model_bl, img_n, 24)\n",
    "    pred1, relevance1 = get_relevance_new(model_tp, img_n, 24)\n",
    "    pred_bl_1, relevance_bl_1 = get_relevance_new(model_bl, img_n, 24)\n",
    "    \n",
    "\n",
    "\n",
    "    img_hm_1 = imgify(relevance1, symmetric=True, cmap='coldnhot')\n",
    "    # img_hm_2 = imgify(relevance2, symmetric=True, cmap='coldnhot')\n",
    "    # img_hm_3 = imgify(relevance3, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "    img_hm_bl_1 = imgify(relevance_bl_1, symmetric=True, cmap='coldnhot')\n",
    "    # img_hm_bl_2 = imgify(relevance_bl_2, symmetric=True, cmap='coldnhot')\n",
    "    # img_hm_bl_3 = imgify(relevance_bl_3, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "    ax[0].set_title(f'Target: {inv_class_map[lbl.item()]}')\n",
    "    ax[1].set_title(f'Tempo: {inv_class_map[pred1.item()]}')\n",
    "    ax[2].set_title(f'Baseline: {inv_class_map[pred_bl_1.item()]}')\n",
    "    # ax[1].set_title(f'Prediction: {inv_class_map[pred1.item()]}')\n",
    "    # ax[2].set_title(f'Prediction: {inv_class_map[pred2.item()]}')\n",
    "    # ax[3].set_title(f'Prediction: {inv_class_map[pred3.item()]}')\n",
    "    # ax[4].set_title(f'Prediction: {inv_class_map[pred_bl_1.item()]}')\n",
    "    # ax[5].set_title(f'Prediction: {inv_class_map[pred_bl_2.item()]}')\n",
    "    # ax[6].set_title(f'Prediction: {inv_class_map[pred_bl_3.item()]}')\n",
    "\n",
    "    im = im[0]\n",
    "    ax[0].imshow(im.permute(1,2,0))\n",
    "    ax[1].imshow(img_hm_1)\n",
    "    ax[2].imshow(img_hm_bl_1)\n",
    "\n",
    "    # ax[1].imshow(img_hm_1)\n",
    "    # ax[2].imshow(img_hm_2)\n",
    "    # ax[3].imshow(img_hm_3)\n",
    "    # ax[4].imshow(img_hm_bl_1)\n",
    "    # ax[5].imshow(img_hm_bl_2)\n",
    "    # ax[6].imshow(img_hm_bl_3)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5af30e9e16ef45cf56fe7212a84c27a65c36d6841edf0bd9661d40f4090c043c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
